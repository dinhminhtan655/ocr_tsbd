# import os
# from typing import List
# import numpy as np
# import torch
# import torchvision.transforms as T
# from PIL import Image
# from torchvision.transforms.functional import InterpolationMode
# from transformers import AutoModel, AutoTokenizer
# import io
# import json

# # --- Th∆∞ vi·ªán cho API ---
# # THAY ƒê·ªîI: Import th√™m 'Form' ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu t·ª´ form
# from fastapi import FastAPI, File, HTTPException, UploadFile, Form
# import uvicorn

# # =================================================================================
# # PH·∫¶N 1: C√ÅC H√ÄM X·ª¨ L√ù ·∫¢NH V√Ä MODEL (Kh√¥ng thay ƒë·ªïi)
# # =================================================================================

# IMAGENET_MEAN = (0.485, 0.456, 0.406)
# IMAGENET_STD = (0.229, 0.224, 0.225)

# def build_transform(input_size):
#     MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
#     transform = T.Compose([
#         T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
#         T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
#         T.ToTensor(),
#         T.Normalize(mean=MEAN, std=STD)
#     ])
#     return transform

# # ... (C√°c h√†m x·ª≠ l√Ω ·∫£nh kh√°c gi·ªØ nguy√™n nh∆∞ c≈©) ...
# def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
#     best_ratio_diff = float('inf')
#     best_ratio = (1, 1)
#     area = width * height
#     for ratio in target_ratios:
#         target_aspect_ratio = ratio[0] / ratio[1]
#         ratio_diff = abs(aspect_ratio - target_aspect_ratio)
#         if ratio_diff < best_ratio_diff:
#             best_ratio_diff = ratio_diff
#             best_ratio = ratio
#         elif ratio_diff == best_ratio_diff:
#             if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
#                 best_ratio = ratio
#     return best_ratio

# def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
#     orig_width, orig_height = image.size
#     aspect_ratio = orig_width / orig_height
#     target_ratios = set(
#         (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
#         i * j <= max_num and i * j >= min_num)
#     target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
#     target_aspect_ratio = find_closest_aspect_ratio(
#         aspect_ratio, target_ratios, orig_width, orig_height, image_size)
#     target_width = image_size * target_aspect_ratio[0]
#     target_height = image_size * target_aspect_ratio[1]
#     blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
#     resized_img = image.resize((target_width, target_height))
#     processed_images = []
#     for i in range(blocks):
#         box = (
#             (i % (target_width // image_size)) * image_size,
#             (i // (target_width // image_size)) * image_size,
#             ((i % (target_width // image_size)) + 1) * image_size,
#             ((i // (target_width // image_size)) + 1) * image_size
#         )
#         split_img = resized_img.crop(box)
#         processed_images.append(split_img)
#     assert len(processed_images) == blocks
#     if use_thumbnail and len(processed_images) != 1:
#         thumbnail_img = image.resize((image_size, image_size))
#         processed_images.append(thumbnail_img)
#     return processed_images

# def load_image(image_file, input_size=448, max_num=12):
#     image = image_file.convert('RGB')
#     transform = build_transform(input_size=input_size)
#     images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
#     pixel_values = [transform(image) for image in images]
#     pixel_values = torch.stack(pixel_values)
#     return pixel_values


# # =================================================================================
# # PH·∫¶N 2: T·∫¢I MODEL V√Ä ƒê·ªäNH NGHƒ®A API
# # =================================================================================

# app = FastAPI(title="S·ªï H·ªìng OCR API", description="API ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin t·ª´ ·∫£nh s·ªï h·ªìng.")

# if torch.backends.mps.is_available():
#     device = torch.device("mps")
#     print("S·ª≠ d·ª•ng GPU (MPS) c·ªßa Apple Silicon.")
# else:
#     device = torch.device("cpu")
#     print("Kh√¥ng t√¨m th·∫•y MPS, s·ª≠ d·ª•ng CPU. Qu√° tr√¨nh x·ª≠ l√Ω c√≥ th·ªÉ ch·∫≠m.")

# model_name = "5CD-AI/Vintern-1B-v3_5"
# print("ƒêang t·∫£i model...")
# try:
#   model = AutoModel.from_pretrained(
#       model_name,
#       torch_dtype=torch.bfloat16,
#       low_cpu_mem_usage=True,
#       trust_remote_code=True,
#       use_flash_attn=False,
#   ).eval().to(device)
# except Exception:
#   model = AutoModel.from_pretrained(
#       model_name,
#       torch_dtype=torch.bfloat16,
#       low_cpu_mem_usage=True,
#       trust_remote_code=True
#   ).eval().to(device)

# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)
# print("Model ƒë√£ t·∫£i xong.")

# # --- ƒê·ªãnh nghƒ©a API Endpoint ---
# # THAY ƒê·ªîI QUAN TR·ªåNG: Th√™m `prompt: str = Form(...)` ƒë·ªÉ nh·∫≠n c·∫£ prompt t·ª´ client
# @app.post("/extract-land-title-info/")
# async def extract_info(files: List[UploadFile] = File(...), prompt: str = Form(...)):
#     """
#     Nh·∫≠n m·ªôt DANH S√ÅCH file ·∫£nh v√† m·ªôt chu·ªói prompt, x·ª≠ l√Ω v√† tr·∫£ v·ªÅ th√¥ng tin.
#     """
#     if not files:
#         raise HTTPException(status_code=400, detail="No files sent.")

#     print(f"Nh·∫≠n ƒë∆∞·ª£c {len(files)} file(s).")
    
#     list_of_pixel_values = []
#     # L·∫∑p qua t·ª´ng file ·∫£nh ƒë∆∞·ª£c g·ª≠i l√™n
#     for file in files:
#         print(f"ƒêang x·ª≠ l√Ω file: {file.filename}")
#         contents = await file.read()
#         image = Image.open(io.BytesIO(contents))

#         # S·ª≠ d·ª•ng l·∫°i h√†m load_image ƒë·ªÉ x·ª≠ l√Ω t·ª´ng ·∫£nh m·ªôt
#         # H√†m n√†y s·∫Ω tr·∫£ v·ªÅ m·ªôt tensor c√°c "m·∫£nh v√°" (patches) cho m·ªói ·∫£nh
#         pixel_values_single_image = load_image(image, max_num=6)
#         list_of_pixel_values.append(pixel_values_single_image)

#     # N·ªëi t·∫•t c·∫£ c√°c tensor t·ª´ c√°c ·∫£nh l·∫°i th√†nh m·ªôt tensor l·ªõn duy nh·∫•t
#     # ƒê√¢y l√† b∆∞·ªõc quan tr·ªçng ƒë·ªÉ model "nh√¨n" th·∫•y t·∫•t c·∫£ c√°c ·∫£nh c√πng l√∫c
#     combined_pixel_values = torch.cat(list_of_pixel_values, dim=0).to(torch.bfloat16).to(device)
    
#     print(f"ƒê√£ g·ªôp c√°c ·∫£nh, t·ªïng s·ªë tensor patches: {combined_pixel_values.shape[0]}")
    
#     # C√¢u h·ªèi prompt kh√¥ng thay ƒë·ªïi
#     question = prompt

#     generation_config = dict(max_new_tokens=512, do_sample=False, num_beams=3, repetition_penalty=3.5)

#     print("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v·ªõi model...")
#     # Truy·ªÅn tensor ƒë√£ ƒë∆∞·ª£c g·ªôp v√†o model
#     response = model.chat(tokenizer, combined_pixel_values, question, generation_config)
#     print(f"Model tr·∫£ v·ªÅ: {response}")

#     try:
#         start_index = response.find('{')
#         end_index = response.rfind('}') + 1
#         json_str = response[start_index:end_index]
#         json_response = json.loads(json_str)
#         return json_response
#     except Exception as e:
#         print(f"Kh√¥ng th·ªÉ parse JSON t·ª´ response c·ªßa model. L·ªói: {e}")
#         return {"error": "Could not parse model response to JSON", "raw_response": response}

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000)


# =========== VERSION 2 =================


# import os
# from typing import List
# import numpy as np
# import torch
# import torchvision.transforms as T
# from PIL import Image
# from torchvision.transforms.functional import InterpolationMode
# from transformers import AutoModel, AutoTokenizer
# import io
# import json
# import time # <-- Th√™m th∆∞ vi·ªán time

# # --- Th∆∞ vi·ªán cho API ---
# from fastapi import FastAPI, File, HTTPException, UploadFile, Form
# import uvicorn

# # =================================================================================
# # PH·∫¶N 1: C√ÅC H√ÄM X·ª¨ L√ù ·∫¢NH V√Ä MODEL (Kh√¥ng thay ƒë·ªïi)
# # =================================================================================

# IMAGENET_MEAN = (0.485, 0.456, 0.406)
# IMAGENET_STD = (0.229, 0.224, 0.225)

# def build_transform(input_size):
#     MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
#     transform = T.Compose([
#         T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
#         T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
#         T.ToTensor(),
#         T.Normalize(mean=MEAN, std=STD)
#     ])
#     return transform

# def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
#     best_ratio_diff = float('inf')
#     best_ratio = (1, 1)
#     area = width * height
#     for ratio in target_ratios:
#         target_aspect_ratio = ratio[0] / ratio[1]
#         ratio_diff = abs(aspect_ratio - target_aspect_ratio)
#         if ratio_diff < best_ratio_diff:
#             best_ratio_diff = ratio_diff
#             best_ratio = ratio
#         elif ratio_diff == best_ratio_diff:
#             if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
#                 best_ratio = ratio
#     return best_ratio

# def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
#     orig_width, orig_height = image.size
#     aspect_ratio = orig_width / orig_height
#     target_ratios = set(
#         (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
#         i * j <= max_num and i * j >= min_num)
#     target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
#     target_aspect_ratio = find_closest_aspect_ratio(
#         aspect_ratio, target_ratios, orig_width, orig_height, image_size)
#     target_width = image_size * target_aspect_ratio[0]
#     target_height = image_size * target_aspect_ratio[1]
#     blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
#     resized_img = image.resize((target_width, target_height))
#     processed_images = []
#     for i in range(blocks):
#         box = (
#             (i % (target_width // image_size)) * image_size,
#             (i // (target_width // image_size)) * image_size,
#             ((i % (target_width // image_size)) + 1) * image_size,
#             ((i // (target_width // image_size)) + 1) * image_size
#         )
#         split_img = resized_img.crop(box)
#         processed_images.append(split_img)
#     assert len(processed_images) == blocks
#     if use_thumbnail and len(processed_images) != 1:
#         thumbnail_img = image.resize((image_size, image_size))
#         processed_images.append(thumbnail_img)
#     return processed_images

# def process_image_to_tensor(image_obj, input_size=448, max_num=12):
#     image = image_obj.convert('RGB')
#     transform = build_transform(input_size=input_size)
#     images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
#     pixel_values = [transform(image) for image in images]
#     pixel_values = torch.stack(pixel_values)
#     return pixel_values


# # =================================================================================
# # PH·∫¶N 2: T·∫¢I MODEL V√Ä ƒê·ªäNH NGHƒ®A API
# # =================================================================================

# app = FastAPI(title="S·ªï H·ªìng OCR API", description="API ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin t·ª´ ·∫£nh s·ªï h·ªìng.")

# if torch.cuda.is_available():
#     device = torch.device("cuda")
#     print("‚úÖ ƒê√£ ph√°t hi·ªán v√† ƒëang s·ª≠ d·ª•ng GPU NVIDIA (CUDA).")
# elif torch.backends.mps.is_available():
#     device = torch.device("mps")
#     print("‚úÖ ƒê√£ ph√°t hi·ªán v√† ƒëang s·ª≠ d·ª•ng GPU Apple Silicon (MPS).")
# else:
#     device = torch.device("cpu")
#     print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GPU, ƒëang s·ª≠ d·ª•ng CPU. Qu√° tr√¨nh x·ª≠ l√Ω s·∫Ω r·∫•t ch·∫≠m.")

# model_name = "5CD-AI/Vintern-1B-v3_5"
# print("ƒêang t·∫£i model...")
# model = AutoModel.from_pretrained(
#     model_name,
#     torch_dtype=torch.bfloat16,
#     low_cpu_mem_usage=True,
#     trust_remote_code=True
# ).eval().to(device)
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)
# print("Model ƒë√£ t·∫£i xong.")

# # --- ƒê·ªãnh nghƒ©a API Endpoint v·ªõi log g·ª° l·ªói ---
# @app.post("/extract-land-title-info/")
# async def extract_info(files: List[UploadFile] = File(...), prompt: str = Form(...)):
#     start_time = time.time()
#     print(f"\n{'='*20} B·∫ÆT ƒê·∫¶U REQUEST M·ªöI {'='*20}")
#     print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 1 - Nh·∫≠n ƒë∆∞·ª£c {len(files)} file(s).")
    
#     if not files:
#         raise HTTPException(status_code=400, detail="No files sent.")
    
#     list_of_pixel_values = []
    
#     # B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p x·ª≠ l√Ω ·∫£nh
#     print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 2 - B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p ti·ªÅn x·ª≠ l√Ω ·∫£nh.")
#     for file in files:
#         print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.1 - ƒêang x·ª≠ l√Ω file: {file.filename}")
        
#         contents = await file.read()
#         print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.2 - ƒê√£ ƒë·ªçc {len(contents)} bytes t·ª´ file.")

#         image = Image.open(io.BytesIO(contents))
#         print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.3 - ƒê√£ m·ªü ·∫£nh b·∫±ng PIL.")
        
#         pixel_values_single_image = process_image_to_tensor(image, max_num=6)
#         list_of_pixel_values.append(pixel_values_single_image)
#         print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.4 - ƒê√£ x·ª≠ l√Ω ·∫£nh th√†nh tensor.")

#     print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 3 - Ho√†n th√†nh v√≤ng l·∫∑p. B·∫Øt ƒë·∫ßu g·ªôp tensor.")
#     combined_pixel_values = torch.cat(list_of_pixel_values, dim=0).to(torch.bfloat16).to(device)
#     print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 4 - ƒê√£ g·ªôp c√°c ·∫£nh v√† chuy·ªÉn sang GPU. T·ªïng s·ªë patches: {combined_pixel_values.shape[0]}")
    
#     question = prompt
#     generation_config = dict(max_new_tokens=2048, do_sample=False, num_beams=3, repetition_penalty=3.5)

#     print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 5 - B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v·ªõi model.chat()...")
#     response = model.chat(tokenizer, combined_pixel_values, question, generation_config)
#     print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 6 - Model ƒë√£ tr·∫£ v·ªÅ response.")
    
#     print(f"Model tr·∫£ v·ªÅ: {response}")

#     print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 7 - B·∫Øt ƒë·∫ßu parse JSON.")
#     try:
#         # ... (Kh·ªëi code parse JSON gi·ªØ nguy√™n nh∆∞ c≈©) ...
#         start_bracket = response.find('[')
#         start_curly = response.find('{')
#         if start_bracket != -1 and (start_curly == -1 or start_bracket < start_curly):
#             start_index = start_bracket
#             end_char = ']'
#         else:
#             start_index = start_curly
#             end_char = '}'
#         if start_index == -1: raise ValueError("Kh√¥ng t√¨m th·∫•y k√Ω t·ª± b·∫Øt ƒë·∫ßu JSON")
#         end_index = response.rfind(end_char)
#         if end_index == -1 or end_index < start_index: raise ValueError("Kh√¥ng t√¨m th·∫•y k√Ω t·ª± k·∫øt th√∫c JSON")
#         json_str = response[start_index : end_index + 1]
#         json_response = json.loads(json_str)
#         print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 8 - Parse JSON th√†nh c√¥ng. Ho√†n t·∫•t request.")
#         return json_response
            
#     except Exception as e:
#         print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 8 FAILED - Kh√¥ng th·ªÉ parse JSON. L·ªói: {e}")
#         return {"error": "Could not parse model response to JSON", "raw_response": response}

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000)
    
    
    
# =============== VERSION 3 ===================

import os
from typing import List
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer
import io
import json
import time

# --- Th∆∞ vi·ªán cho API ---
from fastapi import FastAPI, File, HTTPException, UploadFile, Form
import uvicorn
from contextlib import asynccontextmanager

# =================================================================================
# PH·∫¶N 1: C√ÅC H√ÄM X·ª¨ L√ù ·∫¢NH V√Ä MODEL (Kh√¥ng thay ƒë·ªïi)
# =================================================================================

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def process_image_to_tensor(image_obj, input_size=448, max_num=12):
    image = image_obj.convert('RGB')
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values


# =================================================================================
# PH·∫¶N 2: T·∫¢I MODEL V√Ä ƒê·ªäNH NGHƒ®A API (ƒê√É S·ª¨A)
# =================================================================================

# T·∫°o m·ªôt dictionary ƒë·ªÉ ch·ª©a model v√† c√°c t√†i nguy√™n li√™n quan
ml_models = {}

# <<< THAY ƒê·ªîI L·ªöN B·∫ÆT ƒê·∫¶U T·ª™ ƒê√ÇY >>>
# S·ª≠ d·ª•ng 'lifespan' ƒë·ªÉ t·∫£i model khi ·ª©ng d·ª•ng (worker) kh·ªüi ƒë·ªông
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ------------------ KH·ªêI L·ªÜNH T·∫¢I MODEL ------------------
    # Code trong n√†y s·∫Ω ch·∫°y M·ªòT L·∫¶N cho M·ªñI WORKER PROCESS khi n√≥ kh·ªüi ƒë·ªông.
    print("üöÄ B·∫Øt ƒë·∫ßu kh·ªüi t·∫°o worker v√† t·∫£i model...")
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print("‚úÖ ƒê√£ ph√°t hi·ªán v√† ƒëang s·ª≠ d·ª•ng GPU NVIDIA (CUDA).")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("‚úÖ ƒê√£ ph√°t hi·ªán v√† ƒëang s·ª≠ d·ª•ng GPU Apple Silicon (MPS).")
    else:
        device = torch.device("cpu")
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y GPU, ƒëang s·ª≠ d·ª•ng CPU. Qu√° tr√¨nh x·ª≠ l√Ω s·∫Ω r·∫•t ch·∫≠m.")

    model_name = "5CD-AI/Vintern-1B-v3_5"
    print(f"ƒêang t·∫£i model '{model_name}'...")
    
    # T·∫£i model v√† tokenizer
    model = AutoModel.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    ).eval().to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)
    
    # L∆∞u v√†o dictionary ƒë·ªÉ c√°c endpoint c√≥ th·ªÉ truy c·∫≠p
    ml_models["model"] = model
    ml_models["tokenizer"] = tokenizer
    ml_models["device"] = device
    
    print("‚úÖ Model ƒë√£ t·∫£i xong v√† s·∫µn s√†ng tr√™n worker n√†y.")
    # ---------------------------------------------------------
    
    yield # ·ª©ng d·ª•ng ch·∫°y ·ªü ƒë√¢y
    
    # D·ªçn d·∫πp t√†i nguy√™n khi ·ª©ng d·ª•ng t·∫Øt (t√πy ch·ªçn)
    ml_models.clear()
    print("‚ÑπÔ∏è ƒê√£ d·ªçn d·∫πp t√†i nguy√™n model.")


app = FastAPI(
    title="S·ªï H·ªìng OCR API", 
    description="API ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin t·ª´ ·∫£nh s·ªï h·ªìng.",
    lifespan=lifespan # <-- G√°n h√†m lifespan cho FastAPI
)


# --- ƒê·ªãnh nghƒ©a API Endpoint v·ªõi log g·ª° l·ªói ---
@app.post("/extract-land-title-info/")
async def extract_info(files: List[UploadFile] = File(...), prompt: str = Form(...)):
    start_time = time.time()
    print(f"\n{'='*20} B·∫ÆT ƒê·∫¶U REQUEST M·ªöI {'='*20}")
    print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 1 - Nh·∫≠n ƒë∆∞·ª£c {len(files)} file(s).")
    
    if not files:
        raise HTTPException(status_code=400, detail="No files sent.")
    
    list_of_pixel_values = []
    
    print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 2 - B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p ti·ªÅn x·ª≠ l√Ω ·∫£nh.")
    for file in files:
        print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.1 - ƒêang x·ª≠ l√Ω file: {file.filename}")
        
        contents = await file.read()
        print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.2 - ƒê√£ ƒë·ªçc {len(contents)} bytes t·ª´ file.")

        image = Image.open(io.BytesIO(contents))
        print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.3 - ƒê√£ m·ªü ·∫£nh b·∫±ng PIL.")
        
        pixel_values_single_image = process_image_to_tensor(image, max_num=6)
        list_of_pixel_values.append(pixel_values_single_image)
        print(f"DEBUG {time.time() - start_time:.2f}s:   B∆∞·ªõc 2.4 - ƒê√£ x·ª≠ l√Ω ·∫£nh th√†nh tensor.")

    print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 3 - Ho√†n th√†nh v√≤ng l·∫∑p. B·∫Øt ƒë·∫ßu g·ªôp tensor.")
    # L·∫•y device t·ª´ dictionary ƒë√£ l∆∞u
    device = ml_models["device"]
    combined_pixel_values = torch.cat(list_of_pixel_values, dim=0).to(torch.bfloat16).to(device)
    print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 4 - ƒê√£ g·ªôp c√°c ·∫£nh v√† chuy·ªÉn sang GPU. T·ªïng s·ªë patches: {combined_pixel_values.shape[0]}")
    
    question = prompt
    generation_config = dict(max_new_tokens=2048, do_sample=False, num_beams=3, repetition_penalty=3.5)

    print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 5 - B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v·ªõi model.chat()...")
    # L·∫•y model v√† tokenizer t·ª´ dictionary
    response = ml_models["model"].chat(ml_models["tokenizer"], combined_pixel_values, question, generation_config)
    print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 6 - Model ƒë√£ tr·∫£ v·ªÅ response.")
    
    print(f"Model tr·∫£ v·ªÅ: {response}")

    print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 7 - B·∫Øt ƒë·∫ßu parse JSON.")
    try:
        start_bracket = response.find('[')
        start_curly = response.find('{')
        if start_bracket != -1 and (start_curly == -1 or start_bracket < start_curly):
            start_index = start_bracket
            end_char = ']'
        else:
            start_index = start_curly
            end_char = '}'
        if start_index == -1: raise ValueError("Kh√¥ng t√¨m th·∫•y k√Ω t·ª± b·∫Øt ƒë·∫ßu JSON")
        end_index = response.rfind(end_char)
        if end_index == -1 or end_index < start_index: raise ValueError("Kh√¥ng t√¨m th·∫•y k√Ω t·ª± k·∫øt th√∫c JSON")
        json_str = response[start_index : end_index + 1]
        json_response = json.loads(json_str)
        print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 8 - Parse JSON th√†nh c√¥ng. Ho√†n t·∫•t request.")
        return json_response
            
    except Exception as e:
        print(f"DEBUG {time.time() - start_time:.2f}s: B∆∞·ªõc 8 FAILED - Kh√¥ng th·ªÉ parse JSON. L·ªói: {e}")
        return {"error": "Could not parse model response to JSON", "raw_response": response}


if __name__ == "__main__":
    # Khi ch·∫°y tr·ª±c ti·∫øp, h√£y tƒÉng timeout
    uvicorn.run(app, host="0.0.0.0", port=8000)